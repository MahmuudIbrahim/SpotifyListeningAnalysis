{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c23cf9f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Day 1 — Build Dataset + EDA (Spotify Streaming History → Modeling Tables)\n",
    "\n",
    "**Goal:** Create clean, enriched datasets from your personal Spotify streaming history (2012–2023) and generate a small set of story-ready EDA visuals.\n",
    "\n",
    "**Outputs written by this notebook**\n",
    "- `data/processed/listens_event_level.parquet` (one row per play)\n",
    "- `data/processed/tracks_modeling_table.parquet` (one row per track; ready for Day 2)\n",
    "- `data/processed/artists_modeling_table.parquet` (one row per artist; optional)\n",
    "- `data/cache/audio_features.parquet` (Spotify track audio features + metadata cache)\n",
    "- `data/cache/artist_genres.json` (Spotify artist genres cache)\n",
    "\n",
    "**Expected runtime:** depends on how many unique tracks/artists you have and your Spotify API rate limits. Caching prevents repeated calls.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup checklist\n",
    "1. Create a Spotify Developer app and set `SPOTIPY_CLIENT_ID`, `SPOTIPY_CLIENT_SECRET`, `SPOTIPY_REDIRECT_URI` as environment variables.\n",
    "2. Install deps: `pip install spotipy pandas numpy matplotlib plotly tqdm python-dotenv pyarrow`\n",
    "3. (Optional) Add a CJK-capable font (Noto Sans CJK) to avoid tofu boxes in plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f264fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "HERE = Path().resolve()\n",
    "ROOT_DIR = HERE\n",
    "while not (ROOT_DIR / \"config.py\").exists() and ROOT_DIR != ROOT_DIR.parent:\n",
    "    ROOT_DIR = ROOT_DIR.parent\n",
    "\n",
    "sys.path.append(str(ROOT_DIR))\n",
    "\n",
    "import config\n",
    "print(\"Loaded config from:\", (ROOT_DIR / \"config.py\"))\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Dict, Any, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional: load .env in local dev\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 160)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c60ad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "PROJECT_DIR = Path('.').resolve()\n",
    "DATA_DIR = PROJECT_DIR / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "CACHE_DIR = DATA_DIR / 'cache'\n",
    "FIG_DIR = PROJECT_DIR / 'reports' / 'figures'\n",
    "\n",
    "for d in [RAW_DIR, PROCESSED_DIR, CACHE_DIR, FIG_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# If your JSON files live elsewhere, set this to that folder.\n",
    "# In this chat environment, they're in /mnt/data; locally you likely keep them in data/raw.\n",
    "DEFAULT_INPUT_DIR = RAW_DIR / \"spotify\"\n",
    "if not DEFAULT_INPUT_DIR.exists():\n",
    "    # fallback for chat sandbox runs\n",
    "    DEFAULT_INPUT_DIR = Path(\"/mnt/data\")\n",
    "\n",
    "\n",
    "print('PROJECT_DIR:', PROJECT_DIR)\n",
    "print('RAW_DIR:', RAW_DIR)\n",
    "print('PROCESSED_DIR:', PROCESSED_DIR)\n",
    "print('CACHE_DIR:', CACHE_DIR)\n",
    "print('FIG_DIR:', FIG_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7992c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CJK-safe plotting helper\n",
    "\n",
    "def set_cjk_font(preferred_fonts: Optional[list[str]] = None) -> Optional[str]:\n",
    "    # Attempt to set a CJK-capable font. Returns chosen font name (or None).\n",
    "    if preferred_fonts is None:\n",
    "        preferred_fonts = [\n",
    "            'Noto Sans CJK JP',\n",
    "            'Noto Sans CJK KR',\n",
    "            'Noto Sans CJK SC',\n",
    "            'Noto Sans JP',\n",
    "            'Arial Unicode MS',\n",
    "        ]\n",
    "\n",
    "    available = {f.name for f in mpl.font_manager.fontManager.ttflist}\n",
    "    for name in preferred_fonts:\n",
    "        if name in available:\n",
    "            mpl.rcParams['font.family'] = name\n",
    "            mpl.rcParams['axes.unicode_minus'] = False\n",
    "            return name\n",
    "\n",
    "    # Fallback: DejaVu is commonly available but may not cover CJK fully\n",
    "    mpl.rcParams['font.family'] = 'DejaVu Sans'\n",
    "    mpl.rcParams['axes.unicode_minus'] = False\n",
    "    return None\n",
    "\n",
    "chosen = set_cjk_font()\n",
    "print('CJK font chosen:' , chosen or '(fallback DejaVu Sans; install Noto Sans CJK for better coverage)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ba7437",
   "metadata": {},
   "source": [
    "## 1) Load + clean streaming history JSONs\n",
    "\n",
    "We will:\n",
    "- Read all `Streaming_History_Audio_*.json` files\n",
    "- Keep only music tracks (exclude podcast episodes)\n",
    "- Parse timestamps, build time features\n",
    "- Keep raw playback facts: `ms_played`, `skipped`, start/end reasons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd4d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_streaming_history(input_dir: Path) -> pd.DataFrame:\n",
    "    files = sorted(input_dir.glob('Streaming_History_Audio_*.json'))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f'No Streaming_History_Audio_*.json found in {input_dir}')\n",
    "\n",
    "    dfs = []\n",
    "    for fp in tqdm(files, desc='Reading streaming history JSON'):\n",
    "        df = pd.read_json(fp)\n",
    "        df['source_file'] = fp.name\n",
    "        dfs.append(df)\n",
    "\n",
    "    out = pd.concat(dfs, ignore_index=True)\n",
    "    return out\n",
    "\n",
    "raw_df = load_streaming_history(DEFAULT_INPUT_DIR)\n",
    "raw_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b233f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_streaming_history(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Keep only music tracks (exclude podcast episodes)\n",
    "    df = df.copy()\n",
    "\n",
    "    # Standardize timestamp\n",
    "    df['ts'] = pd.to_datetime(df['ts'], utc=True, errors='coerce')\n",
    "\n",
    "    # Spotify exports sometimes include episodes; keep rows with track URIs\n",
    "    df = df[df['spotify_track_uri'].notna()].copy()\n",
    "\n",
    "    # Keep key identifiers\n",
    "    df.rename(columns={\n",
    "        'master_metadata_track_name': 'track_name',\n",
    "        'master_metadata_album_artist_name': 'artist_name',\n",
    "        'master_metadata_album_album_name': 'album_name',\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Basic sanity\n",
    "    df['ms_played'] = pd.to_numeric(df['ms_played'], errors='coerce')\n",
    "    df = df[df['ms_played'].notna()].copy()\n",
    "    df = df[df['ms_played'] > 0].copy()\n",
    "\n",
    "    # Time features\n",
    "    df['date'] = df['ts'].dt.date\n",
    "    df['year'] = df['ts'].dt.year\n",
    "    df['month'] = df['ts'].dt.month\n",
    "    df['year_month'] = df['ts'].dt.to_period('M').astype(str)\n",
    "    df['dayofweek'] = df['ts'].dt.dayofweek\n",
    "    df['hour'] = df['ts'].dt.hour\n",
    "\n",
    "    # Playback features\n",
    "    df['seconds_played'] = df['ms_played'] / 1000.0\n",
    "\n",
    "    # Ensure boolean-ish skipped field\n",
    "    if 'skipped' in df.columns:\n",
    "        df['skipped'] = df['skipped'].fillna(False).astype(bool)\n",
    "    else:\n",
    "        df['skipped'] = False\n",
    "\n",
    "    # Minimal columns for event-level modeling\n",
    "    keep = [\n",
    "        'ts','date','year','month','year_month','dayofweek','hour',\n",
    "        'track_name','artist_name','album_name','spotify_track_uri',\n",
    "        'ms_played','seconds_played','skipped',\n",
    "        'reason_start','reason_end','shuffle','platform','conn_country','source_file'\n",
    "    ]\n",
    "    keep = [c for c in keep if c in df.columns]\n",
    "    df = df[keep].copy()\n",
    "\n",
    "    return df\n",
    "\n",
    "df = clean_streaming_history(raw_df)\n",
    "print('rows:', len(df))\n",
    "print('unique tracks:', df['spotify_track_uri'].nunique())\n",
    "print('unique artists:', df['artist_name'].nunique())\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bed76f",
   "metadata": {},
   "source": [
    "## 2) Spotify API enrichment (cached)\n",
    "\n",
    "We will add:\n",
    "- Track metadata: `track_id`, `track_popularity`, `duration_ms`, `explicit`, `album_release_date`\n",
    "- Audio features: danceability, energy, valence, tempo, etc.\n",
    "\n",
    "We cache results to `data/cache/audio_features.parquet`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f997cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "sp = spotipy.Spotify(\n",
    "    auth_manager=SpotifyClientCredentials(\n",
    "        client_id=config.SPOTIPY_CLIENT_ID,\n",
    "        client_secret=config.SPOTIPY_CLIENT_SECRET\n",
    "    )\n",
    ")\n",
    "\n",
    "# Quick ping\n",
    "res = sp.search(q=\"test\", limit=1)\n",
    "print(\"Spotify API OK. Sample track:\", res[\"tracks\"][\"items\"][0][\"name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8e7d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_CACHE_FP = CACHE_DIR / 'audio_features.parquet'\n",
    "\n",
    "def uri_to_id(uri: str) -> str:\n",
    "    # spotify:track:<id>\n",
    "    return uri.split(':')[-1]\n",
    "\n",
    "def fetch_track_metadata_and_audio_features(track_uris: Iterable[str], batch_size: int = 50) -> pd.DataFrame:\n",
    "    # Returns a dataframe keyed by spotify_track_uri\n",
    "    track_ids = [uri_to_id(u) for u in track_uris]\n",
    "\n",
    "    rows: list[dict[str, Any]] = []\n",
    "\n",
    "    for i in tqdm(range(0, len(track_ids), batch_size), desc='Spotify tracks()'):\n",
    "        batch_ids = track_ids[i:i+batch_size]\n",
    "        batch_uris = track_uris[i:i+batch_size]\n",
    "\n",
    "        tracks_resp = sp.tracks(batch_ids)\n",
    "        tracks = tracks_resp.get('tracks', [])\n",
    "\n",
    "        # Audio features endpoint also supports up to 100\n",
    "        feats = sp.audio_features(batch_ids)\n",
    "        feats_by_id = {f['id']: f for f in feats if f and f.get('id')}\n",
    "\n",
    "        for uri, t in zip(batch_uris, tracks):\n",
    "            if not t:\n",
    "                continue\n",
    "            tid = t.get('id')\n",
    "            album = t.get('album') or {}\n",
    "\n",
    "            row = {\n",
    "                'spotify_track_uri': uri,\n",
    "                'track_id': tid,\n",
    "                'track_popularity': t.get('popularity'),\n",
    "                'duration_ms': t.get('duration_ms'),\n",
    "                'explicit': t.get('explicit'),\n",
    "                'album_release_date': album.get('release_date'),\n",
    "                'album_release_date_precision': album.get('release_date_precision'),\n",
    "                'artist_id_primary': (t.get('artists') or [{}])[0].get('id'),\n",
    "                'artist_name_primary': (t.get('artists') or [{}])[0].get('name'),\n",
    "            }\n",
    "\n",
    "            af = feats_by_id.get(tid)\n",
    "            if af:\n",
    "                # Select a stable subset\n",
    "                for k in [\n",
    "                    'danceability','energy','key','loudness','mode','speechiness','acousticness',\n",
    "                    'instrumentalness','liveness','valence','tempo','time_signature'\n",
    "                ]:\n",
    "                    row[k] = af.get(k)\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Build/refresh cache\n",
    "unique_uris = sorted(df['spotify_track_uri'].dropna().unique().tolist())\n",
    "\n",
    "if AUDIO_CACHE_FP.exists():\n",
    "    audio_cache = pd.read_parquet(AUDIO_CACHE_FP)\n",
    "    cached_uris = set(audio_cache['spotify_track_uri'].unique())\n",
    "    missing_uris = [u for u in unique_uris if u not in cached_uris]\n",
    "    print(f'Audio cache found: {len(cached_uris)} tracks cached; {len(missing_uris)} missing')\n",
    "else:\n",
    "    audio_cache = pd.DataFrame()\n",
    "    missing_uris = unique_uris\n",
    "    print('No audio cache found; fetching all tracks...')\n",
    "\n",
    "if missing_uris:\n",
    "    fetched = fetch_track_metadata_and_audio_features(missing_uris)\n",
    "    audio_cache = pd.concat([audio_cache, fetched], ignore_index=True)\n",
    "    audio_cache = audio_cache.drop_duplicates(subset=['spotify_track_uri'], keep='last')\n",
    "    audio_cache.to_parquet(AUDIO_CACHE_FP, index=False)\n",
    "    print('Wrote:', AUDIO_CACHE_FP)\n",
    "\n",
    "print('audio_cache rows:', len(audio_cache))\n",
    "audio_cache.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfafd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge enrichment into event-level df\n",
    "\n",
    "df_enriched = df.merge(audio_cache, on='spotify_track_uri', how='left')\n",
    "\n",
    "# Listen ratio (more defensible than a fixed 30-second rule)\n",
    "# Note: duration_ms can be missing for some tracks; handle safely\n",
    "\n",
    "df_enriched['listen_ratio'] = df_enriched['ms_played'] / df_enriched['duration_ms']\n",
    "df_enriched['listen_ratio'] = df_enriched['listen_ratio'].replace([np.inf, -np.inf], np.nan)\n",
    "# Clip extreme ratios (some exports can exceed duration slightly)\n",
    "df_enriched['listen_ratio'] = df_enriched['listen_ratio'].clip(lower=0, upper=1.5)\n",
    "\n",
    "# Buckets for narrative + modeling\n",
    "bins = [-0.01, 0.2, 0.8, 1.5]\n",
    "labels = ['skip_early', 'partial', 'complete']\n",
    "df_enriched['listen_bucket'] = pd.cut(df_enriched['listen_ratio'], bins=bins, labels=labels)\n",
    "\n",
    "print(df_enriched[['seconds_played','duration_ms','listen_ratio','listen_bucket']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819abae8",
   "metadata": {},
   "source": [
    "## 3) Artist genres (cached)\n",
    "\n",
    "Spotify genres are usually at the **artist** level. We'll:\n",
    "- collect primary artist IDs from track metadata\n",
    "- call `artists()` in batches\n",
    "- cache to `data/cache/artist_genres.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ee0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENRE_CACHE_FP = CACHE_DIR / 'artist_genres.json'\n",
    "\n",
    "def fetch_artist_genres(artist_ids: list[str], batch_size: int = 50) -> dict[str, list[str]]:\n",
    "    out: dict[str, list[str]] = {}\n",
    "    artist_ids = [a for a in artist_ids if isinstance(a, str) and a]\n",
    "\n",
    "    for i in tqdm(range(0, len(artist_ids), batch_size), desc='Spotify artists()'):\n",
    "        batch = artist_ids[i:i+batch_size]\n",
    "        resp = sp.artists(batch)\n",
    "        for a in resp.get('artists', []) or []:\n",
    "            if not a:\n",
    "                continue\n",
    "            out[a.get('id')] = a.get('genres') or []\n",
    "\n",
    "    return out\n",
    "\n",
    "# Load cache if exists\n",
    "if GENRE_CACHE_FP.exists():\n",
    "    artist_genres = json.load(open(GENRE_CACHE_FP, 'r', encoding='utf-8'))\n",
    "else:\n",
    "    artist_genres = {}\n",
    "\n",
    "artist_ids = sorted(set(df_enriched['artist_id_primary'].dropna().astype(str)))\n",
    "missing_ids = [a for a in artist_ids if a not in artist_genres]\n",
    "print('artists total:', len(artist_ids), 'missing:', len(missing_ids))\n",
    "\n",
    "if missing_ids:\n",
    "    new_map = fetch_artist_genres(missing_ids)\n",
    "    artist_genres.update(new_map)\n",
    "    with open(GENRE_CACHE_FP, 'w', encoding='utf-8') as f:\n",
    "        json.dump(artist_genres, f, ensure_ascii=False, indent=2)\n",
    "    print('Wrote:', GENRE_CACHE_FP)\n",
    "\n",
    "# Add genres to enriched df\n",
    "\n",
    "def primary_genre(genres: list[str]) -> str:\n",
    "    if not genres:\n",
    "        return 'unknown'\n",
    "    return genres[0]\n",
    "\n",
    "df_enriched['artist_genres'] = df_enriched['artist_id_primary'].map(artist_genres)\n",
    "df_enriched['artist_genres'] = df_enriched['artist_genres'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "df_enriched['primary_genre'] = df_enriched['artist_genres'].apply(primary_genre)\n",
    "\n",
    "df_enriched[['artist_name','artist_name_primary','primary_genre']].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895154cc",
   "metadata": {},
   "source": [
    "## 4) Build modeling tables\n",
    "\n",
    "### Event-level table\n",
    "One row per play (keeps context like hour, platform, reason_end).\n",
    "\n",
    "### Track-level table (Day 2 ready)\n",
    "One row per track with aggregated behavior + content features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f710568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event-level exports\n",
    "EVENT_FP = PROCESSED_DIR / 'listens_event_level.parquet'\n",
    "\n",
    "# Keep an explicit ordered set of columns\n",
    "event_cols = [\n",
    "    'ts','date','year','month','year_month','dayofweek','hour',\n",
    "    'spotify_track_uri','track_id','track_name','artist_name','artist_id_primary','artist_name_primary','album_name',\n",
    "    'ms_played','seconds_played','duration_ms','listen_ratio','listen_bucket','skipped',\n",
    "    'primary_genre','artist_genres',\n",
    "    'danceability','energy','valence','tempo','acousticness','instrumentalness','liveness','speechiness','loudness','mode','key','time_signature',\n",
    "    'track_popularity','album_release_date',\n",
    "    'reason_start','reason_end','shuffle','platform','conn_country','source_file'\n",
    "]\n",
    "event_cols = [c for c in event_cols if c in df_enriched.columns]\n",
    "\n",
    "listens_event = df_enriched[event_cols].copy()\n",
    "listens_event.to_parquet(EVENT_FP, index=False)\n",
    "print('Wrote:', EVENT_FP)\n",
    "listens_event.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c402123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track-level modeling table\n",
    "TRACK_FP = PROCESSED_DIR / 'tracks_modeling_table.parquet'\n",
    "\n",
    "agg = {\n",
    "    'ts': ['min','max'],\n",
    "    'ms_played': ['count','sum','mean'],\n",
    "    'listen_ratio': ['mean','median'],\n",
    "    'skipped': ['mean'],\n",
    "    'seconds_played': ['mean','median'],\n",
    "    'duration_ms': ['first'],\n",
    "}\n",
    "\n",
    "track_df = df_enriched.groupby('spotify_track_uri').agg(agg)\n",
    "track_df.columns = ['_'.join([c for c in col if c]) for col in track_df.columns.values]\n",
    "track_df = track_df.reset_index()\n",
    "\n",
    "# Rename for clarity\n",
    "track_df = track_df.rename(columns={\n",
    "    'ms_played_count': 'play_count',\n",
    "    'ms_played_sum': 'total_ms_played',\n",
    "    'ms_played_mean': 'avg_ms_played',\n",
    "    'skipped_mean': 'skip_rate',\n",
    "    'ts_min': 'first_played_ts',\n",
    "    'ts_max': 'last_played_ts',\n",
    "    'listen_ratio_mean': 'avg_listen_ratio',\n",
    "    'listen_ratio_median': 'median_listen_ratio',\n",
    "    'seconds_played_mean': 'avg_seconds_played',\n",
    "    'seconds_played_median': 'median_seconds_played',\n",
    "    'duration_ms_first': 'duration_ms'\n",
    "})\n",
    "\n",
    "# Join static metadata/audio features (use audio_cache to avoid duplication)\n",
    "meta_cols = [\n",
    "    'spotify_track_uri','track_id','artist_id_primary','artist_name_primary',\n",
    "    'track_popularity','duration_ms','explicit','album_release_date','album_release_date_precision',\n",
    "    'danceability','energy','key','loudness','mode','speechiness','acousticness','instrumentalness','liveness','valence','tempo','time_signature'\n",
    "]\n",
    "meta_cols = [c for c in meta_cols if c in audio_cache.columns]\n",
    "track_meta = audio_cache[meta_cols].drop_duplicates(subset=['spotify_track_uri'])\n",
    "\n",
    "track_df = track_df.merge(track_meta, on='spotify_track_uri', how='left')\n",
    "\n",
    "# Add a representative name/album from event df (most common)\n",
    "name_df = (df_enriched.groupby('spotify_track_uri')\n",
    "           .agg(track_name=('track_name', lambda s: s.mode().iloc[0] if not s.mode().empty else s.iloc[0]),\n",
    "                artist_name=('artist_name', lambda s: s.mode().iloc[0] if not s.mode().empty else s.iloc[0]),\n",
    "                album_name=('album_name', lambda s: s.mode().iloc[0] if not s.mode().empty else s.iloc[0]),\n",
    "                primary_genre=('primary_genre', lambda s: s.mode().iloc[0] if not s.mode().empty else 'unknown'))\n",
    "           .reset_index())\n",
    "\n",
    "track_df = track_df.merge(name_df, on='spotify_track_uri', how='left')\n",
    "\n",
    "# Simple recency (days since last play) for Day 2 ranking\n",
    "now = pd.Timestamp.utcnow()\n",
    "track_df['days_since_last_play'] = (now - pd.to_datetime(track_df['last_played_ts'], utc=True)).dt.total_seconds() / (3600*24)\n",
    "\n",
    "# Save\n",
    "track_df.to_parquet(TRACK_FP, index=False)\n",
    "print('Wrote:', TRACK_FP)\n",
    "track_df.sort_values('play_count', ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff236be",
   "metadata": {},
   "source": [
    "## 5) EDA: minimum set of story-ready visuals\n",
    "\n",
    "We will generate four visuals that are strong “blog/dashboard starters”:\n",
    "1. Listening volume over time (monthly)\n",
    "2. Top artists by play count\n",
    "3. Listen ratio distribution\n",
    "4. Genre share over time (top genres)\n",
    "\n",
    "All plots should display CJK titles safely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc4280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Listening volume over time\n",
    "plt.figure(figsize=(12,4))\n",
    "monthly = listens_event.groupby('year_month').size().sort_index()\n",
    "monthly.plot()\n",
    "plt.title('Listening volume over time (monthly plays)')\n",
    "plt.xlabel('Year-Month')\n",
    "plt.ylabel('Plays')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "fp = FIG_DIR / '01_listening_volume_monthly.png'\n",
    "plt.savefig(fp, dpi=200)\n",
    "print('Saved:', fp)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53eb217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Top artists by play count\n",
    "plt.figure(figsize=(10,5))\n",
    "top_artists = listens_event['artist_name'].value_counts().head(15)[::-1]\n",
    "top_artists.plot(kind='barh')\n",
    "plt.title('Top artists by play count')\n",
    "plt.xlabel('Plays')\n",
    "plt.tight_layout()\n",
    "fp = FIG_DIR / '02_top_artists.png'\n",
    "plt.savefig(fp, dpi=200)\n",
    "print('Saved:', fp)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f184a73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Listen ratio distribution\n",
    "plt.figure(figsize=(10,4))\n",
    "vals = listens_event['listen_ratio'].dropna()\n",
    "plt.hist(vals, bins=50)\n",
    "plt.title('Distribution of listen_ratio (ms_played / duration_ms)')\n",
    "plt.xlabel('listen_ratio')\n",
    "plt.ylabel('count')\n",
    "plt.tight_layout()\n",
    "fp = FIG_DIR / '03_listen_ratio_distribution.png'\n",
    "plt.savefig(fp, dpi=200)\n",
    "print('Saved:', fp)\n",
    "plt.show()\n",
    "\n",
    "print(listens_event['listen_bucket'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b4d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Genre share over time (top 8 genres by volume)\n",
    "\n",
    "# Prepare top genres\n",
    "counts = listens_event['primary_genre'].value_counts()\n",
    "top_genres = counts.head(8).index.tolist()\n",
    "\n",
    "d = listens_event.copy()\n",
    "d['genre_for_plot'] = d['primary_genre'].where(d['primary_genre'].isin(top_genres), other='other')\n",
    "\n",
    "pivot = (d.pivot_table(index='year_month', columns='genre_for_plot', values='spotify_track_uri', aggfunc='count')\n",
    "         .fillna(0)\n",
    "         .sort_index())\n",
    "\n",
    "share = pivot.div(pivot.sum(axis=1), axis=0)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.stackplot(share.index, share.T.values, labels=share.columns)\n",
    "plt.title('Genre share over time (top genres)')\n",
    "plt.xlabel('Year-Month')\n",
    "plt.ylabel('Share of plays')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.02, 1.0))\n",
    "plt.tight_layout()\n",
    "fp = FIG_DIR / '04_genre_share_over_time.png'\n",
    "plt.savefig(fp, dpi=200)\n",
    "print('Saved:', fp)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77a597f",
   "metadata": {},
   "source": [
    "## Day 1 ✅ Exit criteria\n",
    "\n",
    "If you got this far, Day 1 is complete:\n",
    "- You have **event-level** and **track-level** modeling tables written to disk\n",
    "- You have **audio features + genres cached**\n",
    "- You generated 4 story-ready visuals\n",
    "\n",
    "Next: Day 2 will build a recommender using the track table (content similarity / TFRS) and evaluate recommendation quality.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
